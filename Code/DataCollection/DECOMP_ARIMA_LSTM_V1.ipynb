{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from plotly import tools\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode (connected = True)\n",
    "import plotly.graph_objs as go\n",
    "## Append the sys path for xgboost\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "import sys\n",
    "sys.path.append (r\"C:\\Users\\martinwg\\xgboost\\python-package\")\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "df = pd.read_csv (\"crypto-markets.csv\", na_values = ['NA', '?'])\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')\n",
    "df['hlc_average'] = (df['high'] + df['low'] + df['close']) / 3\n",
    "df['ohlc_average'] = (df['open'] + df['high'] + df['low'] + df['close']) / 4\n",
    "bitcoin = df[df['name'] == 'Bitcoin'].copy()\n",
    "bitcoin['target'] = bitcoin['close'].shift(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 1766 observations.\n",
      "Test set has 100 observations.\n"
     ]
    }
   ],
   "source": [
    "cutIdx = len(bitcoin[bitcoin['date']< dt.date(2018, 2, 27)])\n",
    "\n",
    "bit_train = bitcoin[:cutIdx] # sets for ARIMA routine\n",
    "bit_holdout = bitcoin[cutIdx:]\n",
    "print(\"Training set has {} observations.\".format(len(bit_train)))\n",
    "print(\"Test set has {} observations.\".format(len(bit_holdout)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Packages and Models\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from matplotlib import pyplot\n",
    "import itertools\n",
    "import warnings\n",
    "import statsmodels.api as sm\n",
    "from sklearn import metrics\n",
    "from matplotlib import pyplot\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from pandas import DataFrame\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nforecast(nPred, low_train, low_test, modType, modArgs, showResults = False):\n",
    "\n",
    "    from matplotlib import pyplot\n",
    "    from sklearn import metrics\n",
    "    from statsmodels.tsa.arima_model import ARIMA\n",
    "\n",
    "    history = [x for x in low_train]\n",
    "    predictions = list()\n",
    "    rmse = list()\n",
    "\n",
    "    for t in range(0, nPred):\n",
    "        if modType == 'arima':\n",
    "            model = ARIMA(history, order = modArgs)\n",
    "        model_fit = model.fit(disp = 0)\n",
    "        yhat = model_fit.forecast()[0]\n",
    "        predictions.append(yhat)\n",
    "        obs = low_test.values[t]\n",
    "        history.append(obs)\n",
    "        if showResults:\n",
    "            print('predicted=%f, expected=%f' % (yhat, obs))\n",
    "\n",
    "    return(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_sequences(seq_size, obs):\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    for i in range(len(obs) - SEQUENCE_SIZE - 1):\n",
    "        #print(i)\n",
    "        window = obs[i:(i + SEQUENCE_SIZE)]\n",
    "        after_window = obs[i + SEQUENCE_SIZE]\n",
    "        window = [[x] for x in window]\n",
    "        #print(\"{} - {}\".format(window,after_window))\n",
    "        x.append(window)\n",
    "        y.append(after_window)\n",
    "\n",
    "    return np.array(x),np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lookback(dataset, look_back=1):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(dataset) - look_back):\n",
    "        a = dataset[i:(i + look_back), 0]\n",
    "        X.append(a)\n",
    "        Y.append(dataset[i + look_back, 0])\n",
    "    return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = bit_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Main Function\n",
    "def ARIMA_LSTM(train, holdout):\n",
    "    training_idx = int(train.shape[0]*0.9)\n",
    "\n",
    "    train_df = train[:training_idx]\n",
    "    test_df = train[training_idx:]\n",
    "\n",
    "    s = pd.Series(train['close'])\n",
    "    low_VC = s.ewm(alpha = 0.6).mean()\n",
    "    high_VC = s - low_VC\n",
    "\n",
    "    low_train = low_VC[:training_idx]\n",
    "    low_test = low_VC[training_idx:]\n",
    "\n",
    "    high_train = high_VC[:training_idx].tolist() # format (y - y_ses) as a list for LSTM routine\n",
    "    high_test = high_VC[training_idx:].tolist()\n",
    "    \n",
    "        \n",
    "    ### ARIMA part on low\n",
    "    model = ARIMA(low_train, order=(5,1,0))\n",
    "    d = range(0, 2)\n",
    "    p = q = range(0, 6)\n",
    "\n",
    "    # Generate all different combinations of p, q and q triplets\n",
    "    pdq = list(itertools.product(p, d, q))\n",
    "    print(pdq)\n",
    "    \n",
    "    \n",
    "    warnings.filterwarnings(\"ignore\") # specify to ignore warning messages\n",
    "    critVals = list()\n",
    "    modArgs = list()\n",
    "    #low_train.head()\n",
    "    for param in pdq:\n",
    "        try:\n",
    "            mod = sm.tsa.statespace.SARIMAX(low_train, order=param)\n",
    "            results = mod.fit()\n",
    "            critVals.append(results.aic)\n",
    "            modArgs.append(param)\n",
    "            print('ARIMA{} - AIC:{}'.format(param, results.aic))\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # State number of time stamps to forecast\n",
    "    nPred = 1\n",
    "\n",
    "    # Retrieve pdq settings for lowest aic\n",
    "    params = modArgs[min(range(len(critVals)), key=critVals.__getitem__)]\n",
    "    print(params)\n",
    "\n",
    "    holdout_df1 = bit_holdout.iloc[0]['close']\n",
    "    holdout_df1\n",
    "\n",
    "    #train_close = pd.Series (train_close)\n",
    "    model = ARIMA(low_VC, order = params)\n",
    "    model_fit = model.fit(disp = 0)\n",
    "    yforecast_arima = model_fit.forecast()[0]\n",
    "\n",
    "    res_arima = DataFrame(model_fit.resid)\n",
    "    yforecast_arima\n",
    "    training_set = high_train\n",
    "    \n",
    "    training_set = np.reshape(training_set, (len(training_set), 1))\n",
    "    #test_set = test_df['close'].values\n",
    "    test_set = high_test\n",
    "    test_set = np.reshape(test_set, (len(test_set), 1))\n",
    "\n",
    "    #scale datasets\n",
    "    scaler = MinMaxScaler()\n",
    "    training_set = scaler.fit_transform(training_set)\n",
    "    test_set = scaler.transform(test_set)\n",
    "\n",
    "    # create datasets which are suitable for time series forecasting\n",
    "    look_back = len(training_set)-1\n",
    "    look_back1 = len(test_set)-1\n",
    "    X_train, Y_train = create_lookback(training_set, 1)\n",
    "    X_test, Y_test = create_lookback(test_set, 1)\n",
    "\n",
    "     # reshape datasets so that they will be ok for the requirements of the LSTM model in Keras\n",
    "    X_train = np.reshape(X_train, (len(X_train), 1, X_train.shape[1]))\n",
    "    X_test = np.reshape(X_test, (len(X_test), 1, X_test.shape[1]))\n",
    "    \n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(256, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    model.add(LSTM(256))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    \n",
    "    # compile and fit the model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    history = model.fit(X_train, Y_train, epochs=100, batch_size=16, shuffle=False,\n",
    "                    validation_data=(X_test, Y_test),\n",
    "                    callbacks = [EarlyStopping(monitor='val_loss', min_delta=5e-5, patience=10, verbose=1)])\n",
    "    return yforecast_arima;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0, 0), (0, 0, 1), (0, 0, 2), (0, 0, 3), (0, 0, 4), (0, 0, 5), (0, 1, 0), (0, 1, 1), (0, 1, 2), (0, 1, 3), (0, 1, 4), (0, 1, 5), (1, 0, 0), (1, 0, 1), (1, 0, 2), (1, 0, 3), (1, 0, 4), (1, 0, 5), (1, 1, 0), (1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4), (1, 1, 5), (2, 0, 0), (2, 0, 1), (2, 0, 2), (2, 0, 3), (2, 0, 4), (2, 0, 5), (2, 1, 0), (2, 1, 1), (2, 1, 2), (2, 1, 3), (2, 1, 4), (2, 1, 5), (3, 0, 0), (3, 0, 1), (3, 0, 2), (3, 0, 3), (3, 0, 4), (3, 0, 5), (3, 1, 0), (3, 1, 1), (3, 1, 2), (3, 1, 3), (3, 1, 4), (3, 1, 5), (4, 0, 0), (4, 0, 1), (4, 0, 2), (4, 0, 3), (4, 0, 4), (4, 0, 5), (4, 1, 0), (4, 1, 1), (4, 1, 2), (4, 1, 3), (4, 1, 4), (4, 1, 5), (5, 0, 0), (5, 0, 1), (5, 0, 2), (5, 0, 3), (5, 0, 4), (5, 0, 5), (5, 1, 0), (5, 1, 1), (5, 1, 2), (5, 1, 3), (5, 1, 4), (5, 1, 5)]\n",
      "ARIMA(0, 0, 0) - AIC:26338.134219722662\n",
      "ARIMA(0, 1, 0) - AIC:15148.74656572134\n",
      "ARIMA(0, 1, 1) - AIC:14888.898675911074\n",
      "ARIMA(0, 1, 2) - AIC:14853.023297955222\n",
      "ARIMA(0, 1, 3) - AIC:14831.33094154232\n",
      "ARIMA(0, 1, 4) - AIC:14833.300917965882\n",
      "ARIMA(0, 1, 5) - AIC:14833.658804750457\n",
      "ARIMA(1, 1, 0) - AIC:14833.901031203543\n",
      "ARIMA(1, 1, 1) - AIC:14833.588082358809\n",
      "ARIMA(1, 1, 2) - AIC:14835.525845800525\n",
      "ARIMA(1, 1, 3) - AIC:14833.275761230405\n",
      "ARIMA(1, 1, 4) - AIC:14816.239805176312\n",
      "ARIMA(1, 1, 5) - AIC:14816.807023472344\n",
      "ARIMA(2, 1, 0) - AIC:14833.705558972124\n",
      "ARIMA(2, 1, 1) - AIC:14835.674252639266\n",
      "ARIMA(2, 1, 3) - AIC:14834.482332961892\n",
      "ARIMA(2, 1, 4) - AIC:14816.753023604404\n",
      "ARIMA(3, 1, 0) - AIC:14835.323465516096\n",
      "ARIMA(3, 1, 1) - AIC:14834.151757418673\n",
      "ARIMA(3, 1, 3) - AIC:14812.891250192422\n",
      "ARIMA(3, 1, 4) - AIC:14820.45126421613\n",
      "ARIMA(3, 1, 5) - AIC:14807.243832241698\n",
      "ARIMA(4, 1, 0) - AIC:14834.815133329157\n",
      "ARIMA(4, 1, 1) - AIC:14834.21961709109\n",
      "ARIMA(5, 1, 0) - AIC:14832.023608480977\n",
      "ARIMA(5, 1, 1) - AIC:14820.009695457928\n",
      "ARIMA(5, 1, 2) - AIC:14836.10388619502\n",
      "(3, 1, 5)\n",
      "Train on 1588 samples, validate on 176 samples\n",
      "Epoch 1/100\n",
      "1588/1588 [==============================] - 3s 2ms/step - loss: 0.0134 - val_loss: 0.7301\n",
      "Epoch 2/100\n",
      "1588/1588 [==============================] - 2s 1ms/step - loss: 0.0026 - val_loss: 0.7293\n",
      "Epoch 3/100\n",
      "1588/1588 [==============================] - 2s 1ms/step - loss: 0.0027 - val_loss: 0.7297\n",
      "Epoch 4/100\n",
      "1588/1588 [==============================] - 2s 994us/step - loss: 0.0027 - val_loss: 0.7302\n",
      "Epoch 5/100\n",
      "1588/1588 [==============================] - 2s 1ms/step - loss: 0.0027 - val_loss: 0.7308\n",
      "Epoch 6/100\n",
      "1588/1588 [==============================] - 2s 970us/step - loss: 0.0027 - val_loss: 0.7315\n",
      "Epoch 7/100\n",
      "1588/1588 [==============================] - 2s 982us/step - loss: 0.0027 - val_loss: 0.7322\n",
      "Epoch 8/100\n",
      "1588/1588 [==============================] - 2s 991us/step - loss: 0.0027 - val_loss: 0.7329\n",
      "Epoch 9/100\n",
      "1588/1588 [==============================] - 2s 989us/step - loss: 0.0027 - val_loss: 0.7336\n",
      "Epoch 10/100\n",
      "1588/1588 [==============================] - 2s 1ms/step - loss: 0.0027 - val_loss: 0.7342\n",
      "Epoch 11/100\n",
      "1588/1588 [==============================] - 2s 1ms/step - loss: 0.0027 - val_loss: 0.7347\n",
      "Epoch 12/100\n",
      "1588/1588 [==============================] - 2s 982us/step - loss: 0.0027 - val_loss: 0.7353\n",
      "Epoch 00012: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([10244.69415185])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ARIMA_LSTM (bit_train, bit_holdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
